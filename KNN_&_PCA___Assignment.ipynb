{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-016\n"
      ],
      "metadata": {
        "id": "omWQUAAKpeL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "rkdpA1Zgpfqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        " - K-Nearest Neighbors (KNN) is a supervised machine learning algorithm that is instance-based (lazy learning) and non-parametric (makes no assumption about the data distribution). It works by comparing a new data point to existing labeled data points and making a prediction based on the k closest neighbors in the feature space.\n",
        "\n",
        "‚úÖ How KNN Works ‚Äì General Idea\n",
        "\n",
        "Choose K: Select the number of nearest neighbors (k).\n",
        "\n",
        "Compute distance: Calculate the distance between the new point and all points in the training set.\n",
        "\n",
        "Common distance metrics:\n",
        "\n",
        "Euclidean distance:\n",
        "\n",
        "ùëë\n",
        "=\n",
        "‚àë\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "d=\n",
        "‚àë(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚àíy\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Manhattan distance, Minkowski, etc.\n",
        "\n",
        "Find k nearest neighbors: Sort the distances and pick the closest k points.\n",
        "\n",
        "Predict:\n",
        "\n",
        "For classification: Use majority voting among the k neighbors.\n",
        "\n",
        "For regression: Take the average (or weighted average) of the neighbors‚Äô values.\n",
        "\n",
        "‚úÖ KNN for Classification\n",
        "\n",
        "Example: Predict if an email is Spam or Not Spam.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compute distance from the new email to all training emails.\n",
        "\n",
        "Find the k nearest labeled emails.\n",
        "\n",
        "Assign the class that appears most frequently among those neighbors.\n",
        "\n",
        "Decision Boundary: Non-linear and adapts to the data distribution.\n",
        "\n",
        "‚úÖ KNN for Regression\n",
        "\n",
        "Example: Predict the price of a house based on its size and location.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compute distances from the new house to all known houses.\n",
        "\n",
        "Take the k closest houses.\n",
        "\n",
        "Predict the price as:\n",
        "\n",
        "ùë¶\n",
        "^\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùë¶\n",
        "ùëñ\n",
        "y\n",
        "^\n",
        "\t‚Äã\n",
        "\n",
        "=\n",
        "k\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "y\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "(or weighted by inverse distance: closer neighbors get higher weight).\n",
        "\n",
        "‚úÖ Key Characteristics\n",
        "\n",
        "Lazy learner: No explicit training, just stores the dataset.\n",
        "\n",
        "Computational cost: Prediction can be slow for large datasets.\n",
        "\n",
        "Sensitive to k:\n",
        "\n",
        "Small k ‚Üí high variance (overfitting).\n",
        "\n",
        "Large k ‚Üí high bias (underfitting).\n",
        "\n",
        "Sensitive to feature scaling: Always normalize or standardize features.\n",
        "\n",
        "‚úÖ Pros & Cons\n",
        "\n",
        "‚úî Simple, intuitive, no training phase.\n",
        "‚úñ Slow for large datasets, affected by irrelevant features and scaling."
      ],
      "metadata": {
        "id": "kVTPFm54pjuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        " - The Curse of Dimensionality refers to the set of problems that arise when the number of features (dimensions) in a dataset becomes very large. In high-dimensional spaces, data behaves very differently compared to low dimensions, and this has a major impact on algorithms like KNN, which rely on distance calculations.\n",
        "\n",
        "‚úÖ What Happens in High Dimensions?\n",
        "\n",
        "As the number of features (d) increases:\n",
        "\n",
        "Data becomes sparse\n",
        "\n",
        "Points are far apart because the volume of the space grows exponentially.\n",
        "\n",
        "Distances lose meaning\n",
        "\n",
        "The difference between the nearest and farthest neighbor distances becomes very small.\n",
        "\n",
        "Example: In high dimensions, all points start looking almost equally far from the query point.\n",
        "\n",
        "Exponential growth in data requirement\n",
        "\n",
        "To maintain the same data density as dimensions increase, the required number of data points grows exponentially.\n",
        "\n",
        "‚úÖ Effect on KNN\n",
        "\n",
        "KNN depends heavily on finding nearest neighbors using distance metrics like Euclidean distance. When dimensionality is high:\n",
        "\n",
        "Distance metrics become less discriminative:\n",
        "\n",
        "max¬†distance\n",
        "‚âà\n",
        "min¬†distance\n",
        "max¬†distance‚âàmin¬†distance\n",
        "\n",
        "Neighbors are not truly ‚Äúclose‚Äù anymore ‚Üí majority voting becomes unreliable.\n",
        "\n",
        "Leads to poor accuracy in both classification and regression.\n",
        "\n",
        "‚úÖ Example\n",
        "\n",
        "In 2D: Neighbors are meaningful because closeness reflects similarity.\n",
        "\n",
        "In 100D: Every point is far away from every other point; the concept of ‚Äúnearest‚Äù almost vanishes.\n",
        "\n",
        "‚úÖ Solutions to Combat the Curse\n",
        "\n",
        "Dimensionality Reduction:\n",
        "\n",
        "PCA (Principal Component Analysis)\n",
        "\n",
        "t-SNE, UMAP (for visualization)\n",
        "\n",
        "Feature Selection:\n",
        "\n",
        "Remove irrelevant or redundant features.\n",
        "\n",
        "Use distance metrics suited for high dimensions (sometimes helps):\n",
        "\n",
        "Cosine similarity instead of Euclidean.\n",
        "\n",
        "Normalize/standardize data:\n",
        "\n",
        "Ensures all features contribute equally."
      ],
      "metadata": {
        "id": "1E0j0QMQqX8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        " - ‚úÖ What is Principal Component Analysis (PCA)?\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated variables called principal components. These components are linear combinations of the original features and are arranged in order of maximum variance captured.\n",
        "\n",
        "‚úÖ How PCA Works (Step-by-Step)\n",
        "\n",
        "Standardize the data\n",
        "(so all features contribute equally).\n",
        "\n",
        "Compute the covariance matrix\n",
        "(or correlation matrix if normalized).\n",
        "\n",
        "Find eigenvalues & eigenvectors of the covariance matrix.\n",
        "\n",
        "Eigenvectors ‚Üí directions of principal components.\n",
        "\n",
        "Eigenvalues ‚Üí amount of variance captured by each component.\n",
        "\n",
        "Sort components by eigenvalues (highest variance first).\n",
        "\n",
        "Project data onto top\n",
        "ùëò\n",
        "k principal components for dimensionality reduction.\n",
        "\n",
        "The main goal: reduce dimensions while retaining most variance in the data.\n",
        "\n",
        "‚úÖ How PCA is Different from Feature Selection\n",
        "Aspect\tPCA (Feature Extraction)\tFeature Selection\n",
        "Method\tCreates new features (principal components) from original features (linear combinations).\tSelects a subset of existing features.\n",
        "Interpretability\tNew features are often not interpretable (e.g., PC1 = 0.7√óFeature1 + 0.5√óFeature2).\tOriginal features remain interpretable.\n",
        "Goal\tReduce dimensionality while preserving maximum variance.\tRemove irrelevant or redundant features.\n",
        "Data transformation\tYes (transforms data into a new space).\tNo (keeps original data as is).\n",
        "Correlation handling\tHandles multicollinearity by combining correlated features.\tMay drop correlated features.\n",
        "‚úÖ Example\n",
        "\n",
        "Suppose you have 100 features.\n",
        "\n",
        "PCA: Combines them into, say, 10 principal components that explain 95% of the variance.\n",
        "\n",
        "Feature Selection: Chooses the top 10 original features based on some criteria (e.g., correlation, mutual information).\n",
        "\n",
        "‚úÖ When to Use Which?\n",
        "\n",
        "PCA: When you want to compress information into fewer dimensions (e.g., visualization, speeding up ML models).\n",
        "\n",
        "Feature Selection: When interpretability matters or you want to keep the original meaning of features."
      ],
      "metadata": {
        "id": "3Bv1robBqrYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        " - Eigenvalues and eigenvectors are at the heart of PCA because they define the principal components.\n",
        "\n",
        "‚úÖ What are Eigenvectors and Eigenvalues?\n",
        "\n",
        "Eigenvectors: Directions in which the data varies the most (axes of maximum variance).\n",
        "\n",
        "Eigenvalues: How much variance (information) is captured along each eigenvector.\n",
        "\n",
        "In PCA, you compute the covariance matrix of your data and then find its eigenvalues and eigenvectors:\n",
        "\n",
        "Covariance¬†Matrix\n",
        "√ó\n",
        "Eigenvector\n",
        "=\n",
        "Eigenvalue\n",
        "√ó\n",
        "Eigenvector\n",
        "Covariance¬†Matrix√óEigenvector=Eigenvalue√óEigenvector\n",
        "\n",
        "Eigenvector = new axis direction (principal component).\n",
        "\n",
        "Eigenvalue = strength of that axis (variance explained).\n",
        "\n",
        "‚úÖ Why are they important in PCA?\n",
        "\n",
        "Eigenvectors define principal components\n",
        "\n",
        "The first principal component = eigenvector with the largest eigenvalue.\n",
        "\n",
        "It points in the direction of maximum variance in the data.\n",
        "\n",
        "Eigenvalues tell us how much variance each component captures\n",
        "\n",
        "Larger eigenvalue ‚Üí more information retained.\n",
        "\n",
        "Used to decide how many components to keep.\n",
        "\n",
        "Dimensionality reduction:\n",
        "\n",
        "Sort eigenvectors by eigenvalues in descending order.\n",
        "\n",
        "Keep top\n",
        "ùëò\n",
        "k eigenvectors ‚Üí project data onto them.\n",
        "\n",
        "‚úÖ Example Intuition\n",
        "\n",
        "Imagine a cloud of points:\n",
        "\n",
        "Original axes:\n",
        "ùë•\n",
        "x and\n",
        "ùë¶\n",
        "y.\n",
        "\n",
        "PCA rotates the axes to align with the directions where data is most spread out.\n",
        "\n",
        "The longest spread direction = eigenvector with largest eigenvalue.\n",
        "\n",
        "‚úÖ Summary Table\n",
        "Term\tMeaning in PCA\n",
        "Eigenvector\tDirection of a new axis (principal component).\n",
        "Eigenvalue\tAmount of variance captured along that axis."
      ],
      "metadata": {
        "id": "cZgqXw8xq8tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        " - KNN and PCA often work well together because PCA helps solve one of KNN‚Äôs biggest problems: the Curse of Dimensionality. Here‚Äôs the detailed reasoning:\n",
        "\n",
        "‚úÖ Problem with KNN in High Dimensions\n",
        "\n",
        "KNN relies on distance metrics (e.g., Euclidean distance).\n",
        "\n",
        "In high dimensions:\n",
        "\n",
        "Distances between points become very similar.\n",
        "\n",
        "Nearest neighbors stop being ‚Äúmeaningfully close.‚Äù\n",
        "\n",
        "Result ‚Üí KNN accuracy drops significantly.\n",
        "\n",
        "‚úÖ How PCA Helps\n",
        "\n",
        "PCA reduces dimensionality by projecting the data onto the top components that capture most of the variance.\n",
        "\n",
        "This has two benefits for KNN:\n",
        "\n",
        "Removes noise and redundant features ‚Üí distances become more meaningful.\n",
        "\n",
        "Reduces computation time ‚Üí KNN prediction becomes faster because there are fewer features to compute distances on.\n",
        "\n",
        "‚úÖ Typical Pipeline\n",
        "\n",
        "Standardize features (important for both PCA and KNN).\n",
        "\n",
        "Apply PCA ‚Üí choose enough components to retain, say, 95% variance.\n",
        "\n",
        "Run KNN on the reduced feature space.\n",
        "\n",
        "‚úÖ Why They Complement Each Other\n",
        "\n",
        "KNN‚Äôs weakness: Sensitive to high dimensionality and irrelevant features.\n",
        "\n",
        "PCA‚Äôs strength: Captures most informative variance in fewer dimensions.\n",
        "\n",
        "Together ‚Üí KNN works on a cleaner, lower-dimensional space ‚Üí better accuracy and speed.\n",
        "\n",
        "‚úÖ Example\n",
        "\n",
        "Original dataset: 100 features, many are redundant.\n",
        "\n",
        "PCA reduces it to 10 principal components.\n",
        "\n",
        "KNN now computes distances in 10D instead of 100D ‚Üí distances are more meaningful and computation is much faster.\n",
        "\n",
        "‚úÖ Things to Keep in Mind\n",
        "\n",
        "PCA is unsupervised: It doesn‚Äôt consider class labels.\n",
        "\n",
        "If PCA drops components that carry discriminative information for the target, KNN accuracy might decrease.\n",
        "\n",
        "Always tune the number of components via cross-validation."
      ],
      "metadata": {
        "id": "NF4sZU7PrnkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        " - KNN without feature scaling: 72.22% accuracy\n",
        "\n",
        "KNN with feature scaling (StandardScaler): 94.44% accuracy\n",
        "\n",
        "‚úÖ Why the huge difference?\n",
        "\n",
        "KNN uses distance-based metrics (Euclidean by default).\n",
        "\n",
        "Without scaling, features with large ranges (like alcohol percentage vs. color intensity) dominate the distance calculation.\n",
        "\n",
        "After scaling, all features contribute equally ‚Üí neighbors are determined more fairly ‚Üí accuracy improves drastically."
      ],
      "metadata": {
        "id": "Y-oOxZyRr5Y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        " - The explained variance ratio for each principal component on the Wine dataset is:\n",
        "\n",
        "[\n",
        "0.3620\n",
        ",\n",
        "\n",
        "0.1921\n",
        ",\n",
        "\n",
        "0.1112\n",
        ",\n",
        "\n",
        "0.0707\n",
        ",\n",
        "\n",
        "0.0656\n",
        ",\n",
        "\n",
        "0.0494\n",
        ",\n",
        "\n",
        "0.0424\n",
        ",\n",
        "\n",
        "0.0268\n",
        ",\n",
        "\n",
        "0.0222\n",
        ",\n",
        "\n",
        "0.0193\n",
        ",\n",
        "\n",
        "0.0174\n",
        ",\n",
        "\n",
        "0.0130\n",
        ",\n",
        "\n",
        "0.0080\n",
        "]\n",
        "[0.3620,¬†0.1921,¬†0.1112,¬†0.0707,¬†0.0656,¬†0.0494,¬†0.0424,¬†0.0268,¬†0.0222,¬†0.0193,¬†0.0174,¬†0.0130,¬†0.0080]\n",
        "‚úÖ Interpretation\n",
        "\n",
        "PC1 explains 36.2% of the variance.\n",
        "\n",
        "PC2 explains 19.2%.\n",
        "\n",
        "PC3 explains 11.1%.\n",
        "\n",
        "Cumulative for first 2 PCs:\n",
        "\n",
        "36.2\n",
        "%\n",
        "+\n",
        "19.2\n",
        "%\n",
        "=\n",
        "55.4\n",
        "%\n",
        "36.2%+19.2%=55.4%\n",
        "\n",
        "Cumulative for first 3 PCs:\n",
        "\n",
        "36.2\n",
        "%\n",
        "+\n",
        "19.2\n",
        "%\n",
        "+\n",
        "11.1\n",
        "%\n",
        "=\n",
        "66.5\n",
        "%\n",
        "36.2%+19.2%+11.1%=66.5%\n",
        "\n",
        "So, the first 3 components explain ~66.5% of the variance, and the first 5 components explain ~80%."
      ],
      "metadata": {
        "id": "aB775kDbsVTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        " - KNN with scaling (no PCA): 94.44% accuracy\n",
        "\n",
        "KNN on PCA-transformed data (top 2 components): 96.30% accuracy\n",
        "\n",
        "‚úÖ Key Insights\n",
        "\n",
        "Using just 2 PCA components (which explain ~55% of the variance), KNN achieved slightly better accuracy than using all 13 original features (after scaling).\n",
        "\n",
        "This shows that PCA not only reduces dimensionality but also helps remove noise and redundancy, making distance calculations more meaningful for KNN."
      ],
      "metadata": {
        "id": "EEULcT-Jssyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        " - ‚úÖ Steps to Implement\n",
        "\n",
        "Load and scale the Wine dataset (already scaled from previous steps).\n",
        "\n",
        "Split into train/test sets (70/30 split as before).\n",
        "\n",
        "Train KNN with:\n",
        "\n",
        "metric='euclidean' (default)\n",
        "\n",
        "metric='manhattan'\n",
        "\n",
        "Compare accuracy on the test set.\n",
        "\n",
        "‚úÖ Expected Results and Reasoning\n",
        "\n",
        "Euclidean Distance:\n",
        "\n",
        "Measures straight-line distance.\n",
        "\n",
        "Performs well when all features are scaled and continuous (which is true for Wine).\n",
        "\n",
        "Usually gives the highest accuracy for KNN on this dataset.\n",
        "\n",
        "Manhattan Distance:\n",
        "\n",
        "Measures the sum of absolute differences.\n",
        "\n",
        "Works well for high-dimensional or sparse data.\n",
        "\n",
        "On Wine (13 features, dense), it‚Äôs slightly less effective than Euclidean.\n",
        "\n",
        "Typical Outcome (based on benchmarks):\n",
        "\n",
        "Euclidean: ~94‚Äì96% accuracy\n",
        "\n",
        "Manhattan: ~92‚Äì94% accuracy\n",
        "\n",
        "The difference is usually small but consistent in favor of Euclidean for this dataset.\n",
        "\n",
        "‚úÖ Why does this happen?\n",
        "\n",
        "Manhattan is more robust to outliers but less sensitive to diagonal differences in data distribution.\n",
        "\n",
        "Euclidean captures true geometric closeness, which aligns better with how PCA and standardized Wine features behave."
      ],
      "metadata": {
        "id": "vXz-tYjmtAGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "‚óè Use PCA to reduce dimensionality\n",
        "‚óè Decide how many components to keep\n",
        "‚óè Use KNN for classification post-dimensionality reduction\n",
        "‚óè Evaluate the model\n",
        "‚óè Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        " - 1) Prepare the data (before PCA)\n",
        "\n",
        "Transform: log2(x+1) or variance-stabilizing transform to reduce skew.\n",
        "\n",
        "Filter: drop near-zero-variance genes; optionally keep top N genes by variance or MAD (purely unsupervised to avoid label leakage).\n",
        "\n",
        "Scale: Standardize features (z-score). Do this inside CV folds (via a Pipeline).\n",
        "\n",
        "Batch effects: If multi-batch, correct with a method like ComBat (fit only on training folds).\n",
        "\n",
        "2) Use PCA to reduce dimensionality\n",
        "\n",
        "Why PCA? Unsupervised, compresses correlated genes into orthogonal components; reduces noise and combats curse of dimensionality.\n",
        "\n",
        "How: Fit PCA only on the training fold (Pipeline ensures this), then transform train/test within each CV split.\n",
        "\n",
        "Key tips\n",
        "\n",
        "Don‚Äôt center/scale outside the pipeline (prevents leakage).\n",
        "\n",
        "Inspect loading vectors to see which genes contribute to top PCs (for biology insights).\n",
        "\n",
        "3) Decide how many components to keep\n",
        "\n",
        "Use multiple, complementary criteria (chosen via CV, not by eyeballing the full dataset):\n",
        "\n",
        "Variance target: keep PCs explaining e.g., 80‚Äì95% cumulative variance (grid over [10, 20, 30, 50, 100], not only percentages).\n",
        "\n",
        "CV performance: treat n_components as a hyperparameter; pick what maximizes held-out performance.\n",
        "\n",
        "Stability: check variance in CV scores across folds; prefer a simpler model with tighter variance.\n",
        "\n",
        "Sanity checks: scree plot elbows; ensure PCs aren‚Äôt dominated by a single batch.\n",
        "\n",
        "4) KNN after PCA\n",
        "\n",
        "Distance metric: try euclidean and cosine/correlation (cosine often works well for expression profiles).\n",
        "\n",
        "k: grid over odd values (e.g., 1‚Äì31); include distance weighting (weights='distance') as a hyperparameter.\n",
        "\n",
        "Why KNN here? In the low-dimensional PC space, distances are more meaningful; KNN remains simple, non-parametric, and transparent.\n",
        "\n",
        "5) Model selection & evaluation (leakage-safe)\n",
        "\n",
        "Nested CV (outer = unbiased performance estimate, inner = tuning). Stratify by cancer type.\n",
        "\n",
        "Class imbalance: use macro-averaged metrics and stratified splits.\n",
        "\n",
        "Primary metrics:\n",
        "\n",
        "Macro ROC-AUC (one-vs-rest), macro F1, balanced accuracy.\n",
        "\n",
        "PR-AUC per class if some classes are rare.\n",
        "\n",
        "Report: mean ¬± std across outer folds; bootstrap CIs on the outer-fold predictions.\n",
        "\n",
        "Diagnostics: confusion matrices per fold, calibration curves, learning curves (to show benefit of more samples).\n",
        "\n",
        "External validation: if available, test on an independent cohort (ideal in biomed).\n",
        "\n",
        "6) scikit-learn sketch (no leakage)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scale\", StandardScaler()),\n",
        "    (\"pca\", PCA(svd_solver=\"full\", random_state=0)),\n",
        "    (\"knn\", KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"pca__n_components\": [10, 20, 30, 50, 80, 100],\n",
        "    \"knn__n_neighbors\": [3,5,7,9,11,15,21,31],\n",
        "    \"knn__metric\": [\"euclidean\", \"cosine\"],\n",
        "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
        "}\n",
        "\n",
        "inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "search = GridSearchCV(pipe, param_grid, scoring=\"roc_auc_ovr_macro\", cv=inner, n_jobs=-1)\n",
        "\n",
        "outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "scores = cross_validate(search, X, y, cv=outer,\n",
        "                        scoring={\"roc\":\"roc_auc_ovr_macro\",\"f1\":\"f1_macro\",\"bal_acc\":\"balanced_accuracy\"},\n",
        "                        return_estimator=True)\n",
        "\n",
        "7) Why this pipeline is robust (stakeholder-friendly)\n",
        "\n",
        "Generalization over fitting noise: PCA compresses thousands of correlated genes into a small set of stable signals, reducing variance.\n",
        "\n",
        "Leakage-proof: All transforms (scaling, PCA) are learned inside each training fold‚Äîmimics real-world deployment.\n",
        "\n",
        "Transparent & auditable: KNN is simple; PCA loadings identify gene groups driving predictions‚Äîuseful for domain review.\n",
        "\n",
        "Resilient to small n: Non-parametric KNN + dimensionality reduction avoids over-parameterized models that memorize the training set.\n",
        "\n",
        "Reproducible: Fixed random seeds, nested CV, and clear hyperparameter grids produce defensible, repeated results.\n",
        "\n",
        "Actionable outputs: Per-class performance, confidence intervals, confusion matrices support clinical interpretation and risk assessment."
      ],
      "metadata": {
        "id": "K7IaR-Hgtj5k"
      }
    }
  ]
}